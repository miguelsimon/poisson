{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from poisson_id_link_model import Problem, Sim, fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We're modelling particle counts at $k$ detectors, caused by an event in a space X, discretized as $X^m$.\n",
    "\n",
    "We assume that we can model a count at a detector $y_i$ as a Poisson distribution dependent on an event vector $x^m$,\n",
    "\n",
    "$$\n",
    "y_i \\sim Poisson(\\lambda_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda_i = \\alpha_i \\cdot x^m + \\beta_i = \\theta_i \\cdot x^m\n",
    "$$\n",
    "\n",
    "where:\n",
    "* the $x^m$ vectors are sparse; they correspond to a point event\n",
    "* the structure of $\\theta_i$ is not clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, say we've got 3 detectors, and we've discretized the space where decomposition events happen into $m=4$ cells.\n",
    "\n",
    "The Poisson means $\\lambda_i$ are given by\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix} \n",
    "\\lambda_0 \\\\\n",
    "\\lambda_1 \\\\\n",
    "\\lambda_2 \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix} \n",
    "\\alpha_{00} & \\alpha_{01} & \\alpha_{02} & \\alpha_{03} & \\beta_0 \\\\\n",
    "\\alpha_{10} & \\alpha_{11} & \\alpha_{12} & \\alpha_{13} & \\beta_1 \\\\\n",
    "\\alpha_{20} & \\alpha_{21} & \\alpha_{22} & \\alpha_{23} & \\beta_2 \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\cdot\n",
    "\\left[\n",
    "\\begin{matrix} \n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "1 \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems intuitive to me; an event in a given cell causes Poisson-distributed counts in a given detector; the dark count rate is taken care of by $\\beta$; rates are linear in the events.\n",
    "\n",
    "So we have 2 problems.\n",
    "\n",
    "### Estimate the parameter vector $\\theta$ given training data $(x, y)$\n",
    "\n",
    "We have training tuples (presumably from the geant4 simulation) and we use them to estimate the $\\theta$ coefficients.\n",
    "\n",
    "### Given $\\theta$ and detector responses $y$, find the most plausible event vector $x^m$\n",
    "\n",
    "As I understand it, in each time slice we're assuming either 0 or 1 decomposition events happen.\n",
    "\n",
    "If our event space has been discretized into $m$ cells, we have $m + 1$ alternative hypotheses $H = \\{\\overrightarrow{0}, \\overrightarrow{e_1}, \\overrightarrow{e_2}, \\ldots, \\overrightarrow{e_m}\\}$ for each time slice that can explain the detector measurements: the unit vectors in $R^m$ and the $0^m$ vector.\n",
    "\n",
    "So if we have a function proportional to the likelihood $l(y \\mid \\theta, x^m)$ implied by our Poisson model, we're looking for\n",
    "\n",
    "$$\n",
    "\\underset{x^m \\in H}{\\text{argmax}} \\, l(y \\mid \\theta, x^m)\n",
    "$$\n",
    "\n",
    "We get an easy $l$ function if we assume indepedence between detectors and simply multiply their likelihoods together.\n",
    "\n",
    "This is where the l1 penalty might come in useful as the $x^m$ vector is very sparse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem characterization\n",
    "\n",
    "Our problems are characterized by:\n",
    "* the number of detectors $y\\_dim$\n",
    "* the number of discretized space voxels $x\\_dim$\n",
    "\n",
    "so let's come up with a dummy problem and try to recover $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.002405 -0.001151 -0.001148]\n"
     ]
    }
   ],
   "source": [
    "theta = np.array([\n",
    "    [1.0, 0.5, 0.1, 0.1, 0.01],\n",
    "    [0.5, 0.1, 1.0, 0.1, 0.01],\n",
    "    [0.1, 1.0, 0.5, 0.1, 0.3], # this one's dcr is high\n",
    "])\n",
    "sim = Sim(theta)\n",
    "xs, ys, eys = [], [], []\n",
    "for i in range(100000):\n",
    "    x, y = sim.sample()\n",
    "    e_y = sim.obj.Ey_given_x(sim.theta, x)\n",
    "    ys.append(y)\n",
    "    eys.append(e_y)\n",
    "    \n",
    "# quick sanity check\n",
    "print(np.mean(ys, axis = 0) - np.mean(eys, axis = 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof of concept solver:\n",
    "* I'm being really braindead about it, not taking advantage of the sparsity of $x$ to get a quadratic speed up and give the solver an easier time\n",
    "* I'm using the scipy [trust-constr solver](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-trustconstr.html) because we have a nonnegativity constraint\n",
    "* I'm also using [autograd](https://github.com/HIPS/autograd) because it's super easy to get the gradient and I can use the built-in hessian vector product to give the solver second-order information (not that I know if it matters)\n",
    "\n",
    "There must be a specialized solver for this somewhere, maybe in [cvxopt](https://cvxopt.org/) or in [The Bible](https://web.stanford.edu/~boyd/cvxbook/), but I try not to look at the Boyd & Vandenberghe book for too long because it makes my brain hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [sim.sample() for _ in range(100)]\n",
    "theta_100, _sol = fit(sim.x_dim, sim.y_dim, samples)\n",
    "frobenius_100 = np.linalg.norm(theta - theta_100, ord=\"fro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it more samples and see if the frobenius norm of $\\theta - \\hat{\\theta}$ goes down as expected as the number of samples goes up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frobenius 100: 0.6413563129043409 1000: 0.16760840702314156\n"
     ]
    }
   ],
   "source": [
    "samples.extend([sim.sample() for _ in range(900)])\n",
    "theta_1000, _sol = fit(sim.x_dim, sim.y_dim, samples)\n",
    "frobenius_1000 = np.linalg.norm(theta - theta_1000, ord=\"fro\")\n",
    "print('frobenius 100: {0} 1000: {1}'.format(frobenius_100, frobenius_1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
